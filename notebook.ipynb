{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Novel Scenification - Tag Analysis Notebook\n",
    "\n",
    "This notebook allows you to run the tag analysis script and explore the resulting data.\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This project analyzes scene usage in English novels circa 1800 by processing custom-tagged HTML files to extract metrics on narrative techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Run Tag Analysis\n",
    "\n",
    "First, let's run the `count_tags.py` script which will:\n",
    "- Process HTML files in `data/input/`\n",
    "- Generate CSV files in `data/counts/`\n",
    "- Create a summary Excel file at `data/tag_counts_summary.xlsx`\n",
    "- Generate Markdown summaries in `data/SUMMARY.md` and `data/SAMPLES.md`\n",
    "\n",
    "Run the cell below to execute the script:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install the library package dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTANT: After installing make sure to RESTART the notebook kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the external tag analysis script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python count_tags.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Import Libraries\n",
    "\n",
    "Now let's import the necessary libraries for data analysis and visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML, Markdown\n",
    "\n",
    "# Configure visualization style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = [12, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Explore Input and Output Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List input HTML files\n",
    "input_files = sorted(glob.glob('data/input/*.html'))\n",
    "print(f\"Found {len(input_files)} HTML files in data/input/\")\n",
    "\n",
    "# Display the first few files and their sizes\n",
    "print(\"\\nInput HTML files:\")\n",
    "for file in input_files[:5]:\n",
    "    size_kb = os.path.getsize(file) / 1024\n",
    "    print(f\"- {os.path.basename(file)} ({size_kb:.1f} KB)\")\n",
    "    \n",
    "if len(input_files) > 5:\n",
    "    print(f\"...and {len(input_files) - 5} more files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List output CSV files\n",
    "output_files = sorted(glob.glob('data/counts/*.csv'))\n",
    "print(f\"Found {len(output_files)} CSV files in data/counts/\")\n",
    "\n",
    "# Display the first few files\n",
    "print(\"\\nOutput CSV files:\")\n",
    "for file in output_files[:5]:\n",
    "    print(f\"- {os.path.basename(file)}\")\n",
    "    \n",
    "if len(output_files) > 5:\n",
    "    print(f\"...and {len(output_files) - 5} more files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load and Display Summary Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the Excel summary file exists\n",
    "excel_path = 'data/tag_counts_summary.xlsx'\n",
    "if os.path.exists(excel_path):\n",
    "    print(f\"Excel summary file exists at: {excel_path}\")\n",
    "    \n",
    "    # Display available sheets\n",
    "    sheets = pd.ExcelFile(excel_path).sheet_names\n",
    "    print(f\"\\nAvailable sheets in the Excel file: {sheets}\")\n",
    "    \n",
    "    # Load the Summary sheet\n",
    "    summary_df = pd.read_excel(excel_path, sheet_name='Summary')\n",
    "    \n",
    "    # Extract book titles from Sheet column (which may contain HYPERLINK formulas)\n",
    "    def extract_title(sheet_name):\n",
    "        if isinstance(sheet_name, str) and '=HYPERLINK' in sheet_name:\n",
    "            import re\n",
    "            match = re.search(r'\"(.+?)\"', sheet_name)\n",
    "            if match:\n",
    "                return match.group(1)\n",
    "        return sheet_name\n",
    "    \n",
    "    summary_df['Title'] = summary_df['Sheet'].apply(extract_title)\n",
    "    \n",
    "    # Display the key columns\n",
    "    display_cols = ['Title', 'Total_Words', 'Total_Tags', 'Chapter_Count', \n",
    "                   'SceneAction_Count', 'SceneAction_Words', \n",
    "                   'SceneDia_Count', 'SceneDia_Words']\n",
    "    \n",
    "    display(summary_df[display_cols])\n",
    "else:\n",
    "    print(\"Excel summary file not found. Please run the tag analysis script first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Basic Metrics\n",
    "\n",
    "Let's create some basic visualizations of the tag data across texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations if summary data is available\n",
    "if 'summary_df' in locals():\n",
    "    # Try to extract years from titles (assuming format starts with year)\n",
    "    # First, convert Title column to string type\n",
    "    summary_df['Title'] = summary_df['Title'].astype(str)\n",
    "    # Now extract the year\n",
    "    summary_df['Year'] = summary_df['Title'].str.extract(r'^(\\d{4})').astype(float).fillna(0).astype(int)\n",
    "    \n",
    "    # Sort by year\n",
    "    sorted_df = summary_df.sort_values('Year')\n",
    "    \n",
    "    # Create a shortened title for display\n",
    "    sorted_df['Short_Title'] = sorted_df['Title'].str.extract(r'^\\d{4}\\s+(.+?)\\s+\\d')\n",
    "    \n",
    "    # If Short_Title extraction failed, use the full Title\n",
    "    sorted_df['Short_Title'] = sorted_df['Short_Title'].fillna(sorted_df['Title'])\n",
    "    \n",
    "    # Plot total word counts\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    ax = sns.barplot(x='Short_Title', y='Total_Words', data=sorted_df)\n",
    "    plt.title('Total Word Count by Text')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Add year labels on top of bars\n",
    "    for i, year in enumerate(sorted_df['Year']):\n",
    "        ax.text(i, 500, str(year), ha='center', fontweight='bold', color='black')\n",
    "        \n",
    "    plt.show()\n",
    "    \n",
    "    # Prepare data for scene type distribution\n",
    "    scene_data = pd.melt(sorted_df, \n",
    "                        id_vars=['Short_Title', 'Year'], \n",
    "                        value_vars=['SceneAction_Words', 'SceneDia_Words'],\n",
    "                        var_name='Scene Type', value_name='Word Count')\n",
    "    \n",
    "    # Clean up scene type names for display\n",
    "    scene_data['Scene Type'] = scene_data['Scene Type'].str.replace('_Words', '')\n",
    "    \n",
    "    # Plot scene type distribution\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    sns.barplot(x='Short_Title', y='Word Count', hue='Scene Type', data=scene_data)\n",
    "    plt.title('Scene Type Distribution (Word Count)')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No summary data available for visualization. Run the tag analysis first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyze Scene Type Percentages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and visualize scene type percentages\n",
    "if 'sorted_df' in locals():\n",
    "    # Calculate percentages\n",
    "    sorted_df['SceneAction_Pct'] = sorted_df['SceneAction_Words'] / sorted_df['Total_Words'] * 100\n",
    "    sorted_df['SceneDia_Pct'] = sorted_df['SceneDia_Words'] / sorted_df['Total_Words'] * 100\n",
    "    sorted_df['Other_Pct'] = 100 - sorted_df['SceneAction_Pct'] - sorted_df['SceneDia_Pct']\n",
    "    \n",
    "    # Create DataFrame for percentages\n",
    "    pct_data = pd.melt(sorted_df, \n",
    "                      id_vars=['Short_Title', 'Year'], \n",
    "                      value_vars=['SceneAction_Pct', 'SceneDia_Pct', 'Other_Pct'],\n",
    "                      var_name='Scene Type', value_name='Percentage')\n",
    "    \n",
    "    # Clean up scene type names\n",
    "    pct_data['Scene Type'] = pct_data['Scene Type'].str.replace('_Pct', '')\n",
    "    \n",
    "    # Plot percentages\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    sns.barplot(x='Short_Title', y='Percentage', hue='Scene Type', data=pct_data)\n",
    "    plt.title('Percentage of Text by Scene Type')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.ylabel('Percentage of Total Words')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate and display average percentages by year\n",
    "    year_groups = sorted_df.groupby('Year')\n",
    "    year_stats = year_groups.agg({\n",
    "        'SceneAction_Pct': 'mean',\n",
    "        'SceneDia_Pct': 'mean',\n",
    "        'Other_Pct': 'mean',\n",
    "        'Short_Title': 'count'\n",
    "    }).reset_index()\n",
    "    \n",
    "    year_stats.rename(columns={'Short_Title': 'Text_Count'}, inplace=True)\n",
    "    \n",
    "    print(\"\\nAverage Scene Type Usage by Year:\")\n",
    "    display(year_stats)\n",
    "    \n",
    "    # Plot trends over time\n",
    "    year_melt = pd.melt(year_stats, \n",
    "                       id_vars=['Year', 'Text_Count'], \n",
    "                       value_vars=['SceneAction_Pct', 'SceneDia_Pct', 'Other_Pct'],\n",
    "                       var_name='Scene Type', value_name='Percentage')\n",
    "    \n",
    "    year_melt['Scene Type'] = year_melt['Scene Type'].str.replace('_Pct', '')\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.lineplot(x='Year', y='Percentage', hue='Scene Type', \n",
    "                marker='o', markersize=10, linewidth=2,\n",
    "                data=year_melt)\n",
    "    \n",
    "    # Add text count labels\n",
    "    for i, row in year_stats.iterrows():\n",
    "        plt.text(row['Year'], 5, f\"n={int(row['Text_Count'])}\", ha='center', fontweight='bold')\n",
    "    \n",
    "    plt.title('Scene Type Usage Trends by Year')\n",
    "    plt.ylabel('Average Percentage')\n",
    "    plt.grid(True, linestyle='--', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Explore Tag Frequency Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and analyze tag frequency data\n",
    "if os.path.exists(excel_path):\n",
    "    try:\n",
    "        # Load the Summary Freq Tags sheet\n",
    "        freq_tags_df = pd.read_excel(excel_path, sheet_name='Summary Freq Tags')\n",
    "        \n",
    "        # Get column names representing tags (those ending with _Count)\n",
    "        tag_cols = [col for col in freq_tags_df.columns if col.endswith('_Count') \n",
    "                   and col not in ['Total_Tags', 'Chapter_Count']]\n",
    "        \n",
    "        # Extract tag names without _Count suffix\n",
    "        tag_names = [col.replace('_Count', '') for col in tag_cols]\n",
    "        \n",
    "        print(f\"Found {len(tag_names)} unique tags in the corpus\")\n",
    "        \n",
    "        # Calculate tag counts across all texts\n",
    "        tag_counts = {}\n",
    "        tag_words = {}\n",
    "        \n",
    "        for tag in tag_names:\n",
    "            count_col = f\"{tag}_Count\"\n",
    "            words_col = f\"{tag}_Words\"\n",
    "            \n",
    "            if count_col in freq_tags_df.columns:\n",
    "                tag_counts[tag] = freq_tags_df[count_col].sum()\n",
    "                \n",
    "            if words_col in freq_tags_df.columns:\n",
    "                tag_words[tag] = freq_tags_df[words_col].sum()\n",
    "        \n",
    "        # Create DataFrames for plotting\n",
    "        count_df = pd.DataFrame(list(tag_counts.items()), columns=['Tag', 'Count'])\n",
    "        count_df = count_df.sort_values('Count', ascending=False).head(15)\n",
    "        \n",
    "        words_df = pd.DataFrame(list(tag_words.items()), columns=['Tag', 'Word Count'])\n",
    "        words_df = words_df.sort_values('Word Count', ascending=False).head(15)\n",
    "        \n",
    "        # Plot top tags by frequency\n",
    "        plt.figure(figsize=(14, 6))\n",
    "        sns.barplot(x='Tag', y='Count', data=count_df)\n",
    "        plt.title('Top 15 Most Frequent Tags')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot top tags by word count\n",
    "        plt.figure(figsize=(14, 6))\n",
    "        sns.barplot(x='Tag', y='Word Count', data=words_df)\n",
    "        plt.title('Top 15 Tags by Word Count')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing tag frequency: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Analyze an Individual Text\n",
    "\n",
    "Let's create a function to analyze tag patterns in a specific text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_text(text_name=None):\n",
    "    \"\"\"Analyze tag distributions in a specific text\n",
    "    \n",
    "    Args:\n",
    "        text_name: Part of the filename to match. If None, will analyze the first file.\n",
    "    \"\"\"\n",
    "    # Get all CSV files\n",
    "    csv_files = glob.glob('data/counts/*.csv')\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(\"No CSV files found. Run the tag analysis first.\")\n",
    "        return None\n",
    "    \n",
    "    # If no text specified, use the first one\n",
    "    if text_name is None:\n",
    "        csv_file = csv_files[0]\n",
    "    else:\n",
    "        # Find matching files\n",
    "        matches = [f for f in csv_files if text_name in f]\n",
    "        if not matches:\n",
    "            print(f\"No files found matching '{text_name}'\")\n",
    "            return None\n",
    "        csv_file = matches[0]\n",
    "    \n",
    "    print(f\"Analyzing: {os.path.basename(csv_file)}\")\n",
    "    \n",
    "    # Load the CSV\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Get document totals\n",
    "    totals = df[df['tag'] == 'totaldoctagswords'].iloc[0]\n",
    "    total_tags = totals['tag_count']\n",
    "    total_words = totals['word_count']\n",
    "    \n",
    "    print(f\"Total tags: {total_tags}\")\n",
    "    print(f\"Total words: {total_words}\")\n",
    "    \n",
    "    # Remove totals row\n",
    "    df = df[df['tag'] != 'totaldoctagswords']\n",
    "    \n",
    "    # Show top tags by frequency\n",
    "    freq_df = df.sort_values('tag_count', ascending=False).head(15)\n",
    "    print(\"\\nTop 15 most frequent tags:\")\n",
    "    display(freq_df[['tag', 'tag_count', 'word_count']])\n",
    "    \n",
    "    # Visualize top tags by frequency\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    sns.barplot(x='tag', y='tag_count', data=freq_df)\n",
    "    plt.title(f'Top Tags by Frequency in {os.path.basename(csv_file)}')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualize top tags by word count\n",
    "    word_df = df.sort_values('word_count', ascending=False).head(15)\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    sns.barplot(x='tag', y='word_count', data=word_df)\n",
    "    plt.title(f'Top Tags by Word Count in {os.path.basename(csv_file)}')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the first text (if available)\n",
    "output_files = glob.glob('data/counts/*.csv')\n",
    "if output_files:\n",
    "    text_df = analyze_text()\n",
    "else:\n",
    "    print(\"No output files found. Please run the tag analysis script first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Analyze Nested Tag Combinations\n",
    "\n",
    "Let's examine compound tags (those with an underscore, indicating nesting) across the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_compound_tags():\n",
    "    \"\"\"Analyze nested tag combinations across all texts\"\"\"\n",
    "    # Load all CSV files\n",
    "    csv_files = glob.glob('data/counts/*.csv')\n",
    "    if not csv_files:\n",
    "        print(\"No CSV files found. Run the tag analysis first.\")\n",
    "        return None\n",
    "    \n",
    "    # Combine data from all files\n",
    "    all_data = []\n",
    "    for csv_file in csv_files:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        text_name = os.path.basename(csv_file).replace('.csv', '')\n",
    "        df['text'] = text_name\n",
    "        all_data.append(df)\n",
    "    \n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    \n",
    "    # Filter for compound tags (containing underscore)\n",
    "    compound_tags = combined_df[combined_df['tag'].str.contains('_')]\n",
    "    \n",
    "    # Sum counts across all texts\n",
    "    tag_totals = compound_tags.groupby('tag').agg({\n",
    "        'tag_count': 'sum',\n",
    "        'word_count': 'sum'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Get top compounds by frequency\n",
    "    top_by_freq = tag_totals.sort_values('tag_count', ascending=False).head(15)\n",
    "    print(\"Top nested tag combinations by frequency:\")\n",
    "    display(top_by_freq)\n",
    "    \n",
    "    # Visualize top compounds\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    sns.barplot(x='tag', y='tag_count', data=top_by_freq)\n",
    "    plt.title('Top Nested Tag Combinations by Frequency')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Get top compounds by word count\n",
    "    top_by_words = tag_totals.sort_values('word_count', ascending=False).head(15)\n",
    "    print(\"\\nTop nested tag combinations by word count:\")\n",
    "    display(top_by_words)\n",
    "    \n",
    "    # Visualize top compounds by word count\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    sns.barplot(x='tag', y='word_count', data=top_by_words)\n",
    "    plt.title('Top Nested Tag Combinations by Word Count')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return compound_tags\n",
    "\n",
    "# Run the analysis\n",
    "compound_data = analyze_compound_tags()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
